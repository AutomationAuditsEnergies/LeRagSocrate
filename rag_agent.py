# rag_agent.py - Version Azure sans ChromaDB

import os
import time
import random
import sqlite3
from datetime import datetime
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import logging

# Configuration du logger
logger = logging.getLogger(__name__)

# Configuration depuis variables d'environnement (Azure friendly)
SECRET_KEY = os.getenv("OPENAI_API_KEY")
DB_PATH = os.getenv("DB_PATH", "/tmp/database.db")  # Azure utilise /tmp

logger.info(f"üîß RAG Agent configur√© - DB: {DB_PATH}")

# Prompt simplifi√© pour guider l'agent avec m√©moire
prompt_template = PromptTemplate(
    input_variables=["conversation_history", "question"],
    template="""
    Tu es un formateur exp√©riment√© et passionn√© qui accompagne des conseillers relation client √† distance. 
    Tu as 15 ans d'exp√©rience dans le domaine et tu adores transmettre ton savoir.
    
    Tu tutoies toujours tes apprenants et tu es bienveillant, encourageant, et parfois un peu taquin (de mani√®re positive).
    Tu utilises des expressions naturelles comme "Alors", "√âcoute", "Tu vois", "D'ailleurs", "Au fait".
    Tu donnes des exemples concrets et des anecdotes quand c'est pertinent.
    
    IMPORTANT: 
    - Utilise l'historique de conversation pour comprendre le contexte
    - Si l'apprenant r√©pond par "oui", "non", "ok", fais r√©f√©rence √† votre discussion pr√©c√©dente
    - Sois naturel et spontan√©, pas robotique
    - N'utilise JAMAIS d'√©mojis dans tes r√©ponses
    - Termine parfois par une question pour relancer la conversation
    - Base-toi sur tes connaissances en formation relation client √† distance

    Historique de la conversation r√©cente :
    {conversation_history}

    Question actuelle de l'apprenant :
    {question}

    R√©ponse d'Alain (formateur exp√©riment√© en relation client) :
    """,
)

# Variable globale pour le LLM
llm = None


def initialize_llm():
    """Initialise le LLM de mani√®re lazy"""
    global llm

    if llm is not None:
        return  # D√©j√† initialis√©

    try:
        if SECRET_KEY:
            logger.info("ü§ñ Configuration OpenAI GPT-4")
            llm = ChatOpenAI(
                model_name="gpt-4",
                temperature=0.8,
                openai_api_key=SECRET_KEY,
                max_tokens=800,
            )
            logger.info("‚úÖ LLM OpenAI configur√©")
        else:
            logger.warning("‚ö†Ô∏è Pas de cl√© OpenAI - r√©ponses simplifi√©es")
            llm = None

    except Exception as e:
        logger.error(f"‚ùå Erreur configuration LLM: {e}")
        llm = None


def init_conversation_table():
    """Initialise la table des conversations si elle n'existe pas"""
    try:
        # Cr√©er le dossier parent si n√©cessaire
        os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                username TEXT,
                role TEXT,
                message TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                session_id TEXT,
                FOREIGN KEY (user_id) REFERENCES logs(id)
            )
        """
        )

        # Index pour optimiser les requ√™tes
        cursor.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_conversations_user_time 
            ON conversations(username, timestamp DESC)
        """
        )

        conn.commit()
        conn.close()
        logger.info("‚úÖ Table conversations initialis√©e")

    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation table: {e}")
        # En cas d'erreur, on continue (la BDD sera cr√©√©e plus tard)


def get_conversation_history(username: str, limit: int = 6) -> str:
    """R√©cup√®re l'historique r√©cent de conversation depuis la BDD"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        # R√©cup√©rer les derniers messages pour cet utilisateur
        cursor.execute(
            """
            SELECT role, message, timestamp 
            FROM conversations 
            WHERE username = ? 
            ORDER BY timestamp DESC 
            LIMIT ?
        """,
            (username, limit),
        )

        rows = cursor.fetchall()
        conn.close()

        if not rows:
            return "D√©but de la conversation."

        # Inverser l'ordre pour avoir chronologique
        rows.reverse()

        formatted_history = []
        for role, message, timestamp in rows:
            # Raccourcir les messages longs pour le contexte
            short_message = message[:150] + "..." if len(message) > 150 else message
            formatted_history.append(f"{role}: {short_message}")

        return "\n".join(formatted_history)

    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration historique: {e}")
        return "D√©but de la conversation."


def add_to_conversation(username: str, role: str, message: str, user_id: int = None):
    """Ajoute un message √† l'historique de conversation en BDD"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT INTO conversations (user_id, username, role, message) 
            VALUES (?, ?, ?, ?)
        """,
            (user_id, username, role, message),
        )

        conn.commit()
        conn.close()

        logger.debug(f"üíæ Message sauv√©: {username} ({role})")

    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde conversation: {e}")


def get_user_stats(username: str) -> dict:
    """R√©cup√®re des stats sur l'utilisateur (questions pos√©es, etc.)"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT 
                COUNT(*) as total_messages,
                COUNT(CASE WHEN role = 'Utilisateur' THEN 1 END) as questions_posees,
                MIN(timestamp) as premiere_interaction,
                MAX(timestamp) as derniere_interaction
            FROM conversations 
            WHERE username = ?
        """,
            (username,),
        )

        row = cursor.fetchone()
        conn.close()

        return {
            "total_messages": row[0] or 0,
            "questions_posees": row[1] or 0,
            "premiere_interaction": row[2],
            "derniere_interaction": row[3],
        }

    except Exception as e:
        logger.error(f"‚ùå Erreur stats utilisateur: {e}")
        return {"total_messages": 0, "questions_posees": 0}


def cleanup_old_conversations(days_old: int = 30):
    """Nettoie les anciennes conversations (optionnel)"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            DELETE FROM conversations 
            WHERE timestamp < datetime('now', '-{} days')
        """.format(
                days_old
            )
        )

        deleted = cursor.rowcount
        conn.commit()
        conn.close()

        logger.info(f"üßπ {deleted} anciennes conversations supprim√©es")

    except Exception as e:
        logger.error(f"‚ùå Erreur nettoyage conversations: {e}")


def rag_answer(
    question: str, username: str = "utilisateur", user_id: int = None
) -> str:
    """Fonction principale RAG - Version simplifi√©e sans ChromaDB"""
    try:
        # Initialisation du LLM seulement
        initialize_llm()

        logger.info(f"üîç Traitement question de {username}: {question[:50]}...")

        # R√©cup√©rer l'historique de conversation depuis la BDD
        conversation_history = get_conversation_history(username)

        # Ajouter la question de l'utilisateur √† la BDD
        add_to_conversation(username, "Utilisateur", question, user_id)

        # G√©n√©rer la r√©ponse avec OpenAI GPT-4
        if llm and SECRET_KEY:
            try:
                # Utiliser le prompt simplifi√© sans contexte vectoriel
                formatted_prompt = prompt_template.format(
                    conversation_history=conversation_history,
                    question=question,
                )

                response = llm.invoke(formatted_prompt)

                # Extraire le texte de la r√©ponse
                if hasattr(response, "content"):
                    result = response.content
                else:
                    result = str(response)

                logger.info(f"‚úÖ R√©ponse GPT-4 g√©n√©r√©e: {len(result)} caract√®res")

            except Exception as e:
                logger.error(f"‚ùå Erreur GPT-4: {e}")
                result = "D√©sol√©, j'ai un probl√®me technique avec mon syst√®me de r√©ponse. Peux-tu reformuler ta question ?"
        else:
            # R√©ponse de fallback sans LLM
            result = "Bonjour ! Je suis Alain, ton formateur en relation client. Pour l'instant, mon syst√®me n'est pas compl√®tement configur√©, mais n'h√©site pas √† me poser tes questions sur la relation client √† distance. Je ferai de mon mieux pour t'aider !"

            logger.info("‚úÖ R√©ponse fallback g√©n√©r√©e (sans LLM)")

        # Ajouter la r√©ponse d'Alain √† la BDD
        add_to_conversation(username, "Professeur", result, user_id)

        # Simulation d'un temps de r√©ponse humain (r√©duit pour Azure)
        if not os.getenv("AZURE_ENV") and not os.getenv("PORT"):  # Seulement en local
            base_delay = random.uniform(0.8, 2.5)
            length_delay = len(result) * random.uniform(0.015, 0.025)
            thinking_delay = random.uniform(0.5, 1.2)
            total_delay = min(base_delay + length_delay + thinking_delay, 6.0)

            logger.debug(f"üí≠ Alain r√©fl√©chit... ({thinking_delay:.1f}s)")
            time.sleep(total_delay)

        logger.info(f"‚úÖ R√©ponse g√©n√©r√©e: {len(result)} caract√®res")
        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur RAG compl√®te: {e}")
        return "Je ne peux pas r√©pondre √† ta question pour le moment. Une erreur technique est survenue."


# Initialiser la table au d√©marrage du module
try:
    init_conversation_table()
except Exception as e:
    logger.error(f"‚ùå Erreur initialisation module: {e}")

# Test rapide (seulement si ex√©cut√© directement)
if __name__ == "__main__":
    logger.info("üß™ Test du RAG sans ChromaDB...")

    username = "TestUser"

    # Test conversation
    response1 = rag_answer("Qu'est-ce que la motivation h√©doniste ?", username)
    print(f"\nüìù Q1: Qu'est-ce que la motivation h√©doniste ?")
    print(f"ü§ñ R1: {response1}")

    response2 = rag_answer("oui", username)
    print(f"\nüìù Q2: oui")
    print(f"ü§ñ R2: {response2}")

    # Afficher les stats
    stats = get_user_stats(username)
    print(f"\nüìä Stats pour {username}: {stats}")

    # Afficher l'historique
    print(f"\nüß† Historique:")
    print(get_conversation_history(username, 10))
